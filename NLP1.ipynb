{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/t8s0kyv52pbfv6q_x3r132rw0000gn/T/ipykernel_17090/118197663.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import torch\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pranjalmishra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pranjalmishra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "df = pd.read_csv('/Users/pranjalmishra/Desktop/python/ipc_sections.csv', encoding='unicode_escape')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingDesc(text):\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    cnt = Counter()\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "\n",
    "    freqwords = set([w for (w, wc) in cnt.most_common(1)])\n",
    "\n",
    "    text = ' '.join([word for word in str(text).split() if word not in freqwords])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    preprocessed_text = ' '.join(filtered_tokens)\n",
    "    return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Description'] = df['Description'].map(lambda s: preprocessingDesc(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipcs = df['Description'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered: I, Amit Kumar, was returning to my home on a bicycle when I met Lokesh, who was drunk, on the way. He abused and attacked me for some old thing, which caused serious injuries to my eyes and nose and on the way he also threatened to kill me\n"
     ]
    }
   ],
   "source": [
    "user_question = input(\"Please enter your text: \")\n",
    "print(\"You entered:\", user_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_descriptions = [preprocess_text(desc) for desc in df.Description]\n",
    "preprocessed_questions = preprocess_text(user_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "description_vectors = vectorizer.fit_transform(preprocessed_descriptions)\n",
    "question_vector = vectorizer.transform([preprocessed_questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(question_vector, description_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_indices = np.where(similarities[0] >= similarity_threshold)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended IPC(s): IPC_278, IPC_307, IPC_322\n"
     ]
    }
   ],
   "source": [
    "recommended_ipcs = df.loc[similar_indices, 'Section']\n",
    "print(\"Recommended IPC(s):\", ', '.join(map(str, recommended_ipcs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pranjalmishra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pranjalmishra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered: I, Amit Kumar, was returning to my home on a bicycle when I met Lokesh, who was drunk, on the way. He abused and attacked me for some old thing, which caused serious injuries to my eyes and nose and on the way he also threatened to kill me\n",
      "Recommended IPC(s): IPC_278, IPC_307, IPC_322\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "df = pd.read_csv('/Users/pranjalmishra/Desktop/python/ipc_sections.csv', encoding='unicode_escape')\n",
    "\n",
    "def preprocessingDesc(text):\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    cnt = Counter()\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    " \n",
    "    freqwords = set([w for (w, wc) in cnt.most_common(1)])\n",
    "    \n",
    "\n",
    "    text = ' '.join([word for word in str(text).split() if word not in freqwords])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    preprocessed_text = ' '.join(filtered_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "df['Description'] = df['Description'].map(lambda s: preprocessingDesc(s))\n",
    "\n",
    "ipcs = df['Description'].tolist()\n",
    "\n",
    "user_question = input(\"Please enter your text: \")\n",
    "print(\"You entered:\", user_question)\n",
    "\n",
    "preprocessed_descriptions = [preprocess_text(desc) for desc in df.Description]\n",
    "preprocessed_questions = preprocess_text(user_question)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "description_vectors = vectorizer.fit_transform(preprocessed_descriptions)\n",
    "question_vector = vectorizer.transform([preprocessed_questions])\n",
    "\n",
    "similarities = cosine_similarity(question_vector, description_vectors)\n",
    "\n",
    "similarity_threshold = 0.1\n",
    "\n",
    "similar_indices = np.where(similarities[0] >= similarity_threshold)[0]\n",
    "\n",
    "recommended_ipcs = df.loc[similar_indices, 'Section']\n",
    "print(\"Recommended IPC(s):\", ', '.join(map(str, recommended_ipcs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pranjalmishra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pranjalmishra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered: I, Amit Kumar, was returning to my home on a bicycle when I met Lokesh, who was drunk, on the way. He abused and attacked me for some old thing, which caused serious injuries to my eyes and nose and on the way he also threatened to kill me\n",
      "Section: IPC_278, Similarity Score: 0.12\n",
      "Section: IPC_307, Similarity Score: 0.11\n",
      "Section: IPC_322, Similarity Score: 0.11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "df = pd.read_csv('/Users/pranjalmishra/Desktop/python/ipc_sections.csv', encoding='unicode_escape')\n",
    "\n",
    "def preprocessingDesc(text):\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    cnt = Counter()\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    " \n",
    "    freqwords = set([w for (w, wc) in cnt.most_common(1)])\n",
    "    \n",
    "    text = ' '.join([word for word in str(text).split() if word not in freqwords])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    preprocessed_text = ' '.join(filtered_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "df['Description'] = df['Description'].map(lambda s: preprocessingDesc(s))\n",
    "\n",
    "ipcs = df['Description'].tolist()\n",
    "\n",
    "user_question = input(\"Please enter your text: \")\n",
    "print(\"You entered:\", user_question)\n",
    "\n",
    "preprocessed_descriptions = [preprocess_text(desc) for desc in df.Description]\n",
    "preprocessed_questions = preprocess_text(user_question)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "description_vectors = vectorizer.fit_transform(preprocessed_descriptions)\n",
    "question_vector = vectorizer.transform([preprocessed_questions])\n",
    "\n",
    "similarities = cosine_similarity(question_vector, description_vectors)\n",
    "\n",
    "similarity_threshold = 0.1\n",
    "\n",
    "similar_indices = np.where(similarities[0] >= similarity_threshold)[0]\n",
    "\n",
    "for index in similar_indices:\n",
    "    section = df.iloc[index]['Section']\n",
    "    score = similarities[0][index]\n",
    "    print(f\"Section: {section}, Similarity Score: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/t8s0kyv52pbfv6q_x3r132rw0000gn/T/ipykernel_21636/758244322.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pranjalmishra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pranjalmishra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered: I, Amit Kumar, was returning to my home on a bicycle when I met Lokesh, who was drunk, on the way. He abused and attacked me for some old thing, which caused serious injuries to my eyes and nose and on the way he also threatened to kill me\n",
      "Recommended IPC(s): IPC_140, IPC_127, IPC_128, IPC_129, IPC_130, IPC_131, IPC_132, IPC_133, IPC_134, IPC_135, IPC_136, IPC_122, IPC_137, IPC_138, IPC_121A, IPC_122, IPC_123, IPC_124, IPC_124A, IPC_125, IPC_126, IPC_140, IPC_153, IPC_153A, IPC_153AA, IPC_153B, IPC_154, IPC_155, IPC_156, IPC_157, IPC_158, IPC_159, IPC_145, IPC_160, IPC_161, IPC_162, IPC_163, IPC_164, IPC_165, IPC_165A, IPC_166, IPC_166A, IPC_166B, IPC_146, IPC_167, IPC_168, IPC_169, IPC_170, IPC_171, IPC_171A, IPC_171B, IPC_171C, IPC_171D, IPC_171E, IPC_147, IPC_171F, IPC_171G, IPC_171H, IPC_171I, IPC_172, IPC_173, IPC_174, IPC_174A, IPC_175, IPC_176, IPC_148, IPC_177, IPC_178, IPC_179, IPC_180, IPC_181, IPC_182, IPC_183, IPC_184, IPC_185, IPC_186, IPC_149, IPC_187, IPC_188, IPC_189, IPC_190, IPC_150, IPC_151, IPC_152, IPC_224, IPC_225, IPC_225A, IPC_225B, IPC_226, IPC_191, IPC_192, IPC_193, IPC_194, IPC_195, IPC_195A, IPC_196, IPC_197, IPC_198, IPC_199, IPC_200, IPC_201, IPC_202, IPC_203, IPC_204, IPC_205, IPC_206, IPC_207, IPC_208, IPC_209, IPC_210, IPC_211, IPC_212, IPC_213, IPC_214, IPC_215, IPC_216, IPC_216A, IPC_216B, IPC_217, IPC_218, IPC_219, IPC_220, IPC_221, IPC_222, IPC_223, IPC_227, IPC_228, IPC_228A, IPC_229, IPC_229A, IPC_230, IPC_231, IPC_232, IPC_233, IPC_234, IPC_235, IPC_236, IPC_237, IPC_238, IPC_239, IPC_240, IPC_241, IPC_242, IPC_243, IPC_244, IPC_245, IPC_246, IPC_247, IPC_248, IPC_249, IPC_250, IPC_251, IPC_252, IPC_253, IPC_254, IPC_255, IPC_256, IPC_257, IPC_258, IPC_259, IPC_260, IPC_261, IPC_262, IPC_263, IPC_263A, IPC_264, IPC_265, IPC_266, IPC_267, IPC_268, IPC_269, IPC_270, IPC_271, IPC_272, IPC_273, IPC_274, IPC_275, IPC_276, IPC_277, IPC_278, IPC_279, IPC_280, IPC_281, IPC_282, IPC_283, IPC_284, IPC_285, IPC_286, IPC_287, IPC_288, IPC_289, IPC_290, IPC_291, IPC_292, IPC_293, IPC_294, IPC_294A, IPC_295, IPC_295A, IPC_296, IPC_297, IPC_298, IPC_299, IPC_29A, IPC_300, IPC_301, IPC_302, IPC_303, IPC_304, IPC_304A, IPC_304B, IPC_305, IPC_306, IPC_307, IPC_308, IPC_309, IPC_310, IPC_311, IPC_312, IPC_313, IPC_314, IPC_315, IPC_316, IPC_317, IPC_318, IPC_319, IPC_320, IPC_321, IPC_322, IPC_323, IPC_324, IPC_325, IPC_326, IPC_326A, IPC_326B, IPC_327, IPC_328, IPC_329, IPC_330, IPC_331, IPC_332, IPC_333, IPC_334, IPC_335, IPC_336, IPC_337, IPC_338, IPC_339, IPC_340, IPC_341, IPC_342, IPC_343, IPC_344, IPC_345, IPC_346, IPC_347, IPC_348, IPC_349, IPC_350, IPC_351, IPC_352, IPC_353, IPC_354, IPC_354A, IPC_354B, IPC_354C, IPC_354D, IPC_355, IPC_356, IPC_357, IPC_358, IPC_359, IPC_360, IPC_361, IPC_362, IPC_363, IPC_363A, IPC_364, IPC_364A, IPC_365, IPC_366, IPC_366A, IPC_366B, IPC_367, IPC_368, IPC_369, IPC_370, IPC_370A, IPC_371, IPC_372, IPC_373, IPC_374, IPC_375, IPC_376, IPC_376A, IPC_376AB, IPC_376B, IPC_376C, IPC_376D, IPC_376DA, IPC_376DB, IPC_376E, IPC_377, IPC_378, IPC_379, IPC_380, IPC_381, IPC_382, IPC_383, IPC_384, IPC_385, IPC_386, IPC_387, IPC_388, IPC_389, IPC_390, IPC_391, IPC_392, IPC_393, IPC_394, IPC_395, IPC_396, IPC_397, IPC_398, IPC_399, IPC_400, IPC_401, IPC_402, IPC_403, IPC_404, IPC_405, IPC_406, IPC_407, IPC_408, IPC_409, IPC_410, IPC_411, IPC_412, IPC_413, IPC_414, IPC_415, IPC_416, IPC_417, IPC_418, IPC_419, IPC_420, IPC_421, IPC_422, IPC_423, IPC_424, IPC_425, IPC_426, IPC_427, IPC_428, IPC_429, IPC_430, IPC_431, IPC_432, IPC_433, IPC_434, IPC_435, IPC_436, IPC_437, IPC_438, IPC_439, IPC_440, IPC_441, IPC_442, IPC_443, IPC_444, IPC_445, IPC_446, IPC_447, IPC_448, IPC_449, IPC_450, IPC_451, IPC_452, IPC_453, IPC_454, IPC_455, IPC_456, IPC_457, IPC_458, IPC_459, IPC_460, IPC_461, IPC_462, IPC_463, IPC_464, IPC_465, IPC_466, IPC_467, IPC_468, IPC_469, IPC_470, IPC_471, IPC_472, IPC_473, IPC_474, IPC_475, IPC_476, IPC_477, IPC_477A, IPC_478, IPC_479, IPC_480, IPC_481, IPC_482, IPC_483, IPC_484, IPC_485, IPC_486, IPC_487, IPC_488, IPC_489, IPC_489A, IPC_489B, IPC_489C, IPC_489D, IPC_489E, IPC_490, IPC_491, IPC_492, IPC_493, IPC_494, IPC_495, IPC_496, IPC_497, IPC_498, IPC_498A, IPC_499, IPC_500, IPC_501, IPC_502, IPC_503, IPC_504, IPC_505, IPC_506, IPC_507, IPC_508, IPC_509, IPC_510, IPC_511\n",
      "Number of Recommended IPC(s): 444\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/pranjalmishra/Desktop/python/ipc_sections.csv', encoding='unicode_escape')\n",
    "\n",
    "# Preprocessing functions\n",
    "def preprocessingDesc(text):\n",
    "    cnt = Counter()\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "    \n",
    "    # Get the most frequent word(s)\n",
    "    freqwords = set([w for (w, wc) in cnt.most_common(1)])\n",
    "    \n",
    "    # Remove the most frequent word(s) from the text\n",
    "    text = ' '.join([word for word in str(text).split() if word not in freqwords])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Apply preprocessing to descriptions in the dataset\n",
    "df['Description'] = df['Description'].map(lambda s: preprocessingDesc(s))\n",
    "\n",
    "# Convert descriptions into a list of lists (for Word2Vec training)\n",
    "sentences = [preprocess_text(desc) for desc in df.Description]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to compute the averaged vector representation of a sentence\n",
    "def sentence_to_avg_vector(sentence, model):\n",
    "    tokens = preprocess_text(sentence)\n",
    "    word_vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    \n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        # Return a zero vector if none of the tokens are in the vocabulary\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Get user input\n",
    "user_question = input(\"Please enter your text: \")\n",
    "print(\"You entered:\", user_question)\n",
    "\n",
    "# Compute the averaged vector for the user's question\n",
    "user_question_vector = sentence_to_avg_vector(user_question, w2v_model)\n",
    "\n",
    "# Compute averaged vectors for all descriptions\n",
    "description_vectors = np.array([sentence_to_avg_vector(desc, w2v_model) for desc in df.Description])\n",
    "\n",
    "# Calculate cosine similarity between the question and all descriptions\n",
    "def cosine_similarity(v1, v2):\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    if norm_v1 == 0 or norm_v2 == 0:\n",
    "        return 0.0\n",
    "    return np.dot(v1, v2) / (norm_v1 * norm_v2)\n",
    "\n",
    "similarities = np.array([cosine_similarity(user_question_vector, desc_vector) for desc_vector in description_vectors])\n",
    "\n",
    "# Define a similarity threshold (adjust this value as needed)\n",
    "similarity_threshold = 0.1\n",
    "\n",
    "# Get indices of descriptions that exceed the similarity threshold\n",
    "similar_indices = np.where(similarities >= similarity_threshold)[0]\n",
    "\n",
    "# Get and print the recommended IPC sections\n",
    "recommended_ipcs = df.loc[similar_indices, 'Section']\n",
    "print(\"Recommended IPC(s):\", ', '.join(map(str, recommended_ipcs)))\n",
    "\n",
    "num_ipcs_recommended = len(recommended_ipcs)\n",
    "\n",
    "# Print the count and the recommended IPC sections\n",
    "print(f\"Number of Recommended IPC(s): {num_ipcs_recommended}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pranjalmishra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pranjalmishra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered: I, Amit Kumar, was returning to my home on a bicycle when I met Lokesh, who was drunk, on the way. He abused and attacked me for some old thing, which caused serious injuries to my eyes and nose and on the way he also threatened to kill me\n",
      "Number of Recommended IPC(s): 2\n",
      "Recommended IPC(s): IPC_278, IPC_307\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/pranjalmishra/Desktop/python/ipc_sections.csv', encoding='unicode_escape')\n",
    "\n",
    "# Preprocessing functions\n",
    "def preprocessingDesc(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply preprocessing to descriptions in the dataset\n",
    "df['Description'] = df['Description'].map(lambda s: preprocessingDesc(s))\n",
    "\n",
    "# Convert the descriptions to a list for processing\n",
    "ipcs = df['Description'].tolist()\n",
    "\n",
    "# Get user input\n",
    "user_question = input(\"Please enter your text: \")\n",
    "print(\"You entered:\", user_question)\n",
    "\n",
    "# Preprocess the user's question\n",
    "preprocessed_question = preprocessingDesc(user_question)\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on descriptions and transform the data into a term-frequency matrix\n",
    "description_vectors = vectorizer.fit_transform(ipcs)\n",
    "\n",
    "# Transform the user's question using the same vectorizer\n",
    "question_vector = vectorizer.transform([preprocessed_question])\n",
    "\n",
    "# Compute cosine similarity between the question and all descriptions\n",
    "similarities = cosine_similarity(question_vector, description_vectors)\n",
    "\n",
    "# Define a similarity threshold (adjust this value as needed)\n",
    "similarity_threshold = 0.1\n",
    "\n",
    "# Get indices of descriptions that exceed the similarity threshold\n",
    "similar_indices = np.where(similarities[0] >= similarity_threshold)[0]\n",
    "\n",
    "# Get the recommended IPC sections\n",
    "recommended_ipcs = df.loc[similar_indices, 'Section']\n",
    "\n",
    "# Calculate the number of recommended IPC sections\n",
    "num_ipcs_recommended = len(recommended_ipcs)\n",
    "\n",
    "# Print the count and the recommended IPC sections\n",
    "print(f\"Number of Recommended IPC(s): {num_ipcs_recommended}\")\n",
    "print(\"Recommended IPC(s):\", ', '.join(map(str, recommended_ipcs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pranjalmishra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pranjalmishra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered: I, Amit Kumar, was returning to my home on a bicycle when I met Lokesh, who was drunk, on the way. He abused and attacked me for some old thing, which caused serious injuries to my eyes and nose and on the way he also threatened to kill me\n",
      "Number of Recommended IPC(s): 444\n",
      "Recommended IPC(s): IPC_140, IPC_127, IPC_128, IPC_129, IPC_130, IPC_131, IPC_132, IPC_133, IPC_134, IPC_135, IPC_136, IPC_122, IPC_137, IPC_138, IPC_121A, IPC_122, IPC_123, IPC_124, IPC_124A, IPC_125, IPC_126, IPC_140, IPC_153, IPC_153A, IPC_153AA, IPC_153B, IPC_154, IPC_155, IPC_156, IPC_157, IPC_158, IPC_159, IPC_145, IPC_160, IPC_161, IPC_162, IPC_163, IPC_164, IPC_165, IPC_165A, IPC_166, IPC_166A, IPC_166B, IPC_146, IPC_167, IPC_168, IPC_169, IPC_170, IPC_171, IPC_171A, IPC_171B, IPC_171C, IPC_171D, IPC_171E, IPC_147, IPC_171F, IPC_171G, IPC_171H, IPC_171I, IPC_172, IPC_173, IPC_174, IPC_174A, IPC_175, IPC_176, IPC_148, IPC_177, IPC_178, IPC_179, IPC_180, IPC_181, IPC_182, IPC_183, IPC_184, IPC_185, IPC_186, IPC_149, IPC_187, IPC_188, IPC_189, IPC_190, IPC_150, IPC_151, IPC_152, IPC_224, IPC_225, IPC_225A, IPC_225B, IPC_226, IPC_191, IPC_192, IPC_193, IPC_194, IPC_195, IPC_195A, IPC_196, IPC_197, IPC_198, IPC_199, IPC_200, IPC_201, IPC_202, IPC_203, IPC_204, IPC_205, IPC_206, IPC_207, IPC_208, IPC_209, IPC_210, IPC_211, IPC_212, IPC_213, IPC_214, IPC_215, IPC_216, IPC_216A, IPC_216B, IPC_217, IPC_218, IPC_219, IPC_220, IPC_221, IPC_222, IPC_223, IPC_227, IPC_228, IPC_228A, IPC_229, IPC_229A, IPC_230, IPC_231, IPC_232, IPC_233, IPC_234, IPC_235, IPC_236, IPC_237, IPC_238, IPC_239, IPC_240, IPC_241, IPC_242, IPC_243, IPC_244, IPC_245, IPC_246, IPC_247, IPC_248, IPC_249, IPC_250, IPC_251, IPC_252, IPC_253, IPC_254, IPC_255, IPC_256, IPC_257, IPC_258, IPC_259, IPC_260, IPC_261, IPC_262, IPC_263, IPC_263A, IPC_264, IPC_265, IPC_266, IPC_267, IPC_268, IPC_269, IPC_270, IPC_271, IPC_272, IPC_273, IPC_274, IPC_275, IPC_276, IPC_277, IPC_278, IPC_279, IPC_280, IPC_281, IPC_282, IPC_283, IPC_284, IPC_285, IPC_286, IPC_287, IPC_288, IPC_289, IPC_290, IPC_291, IPC_292, IPC_293, IPC_294, IPC_294A, IPC_295, IPC_295A, IPC_296, IPC_297, IPC_298, IPC_299, IPC_29A, IPC_300, IPC_301, IPC_302, IPC_303, IPC_304, IPC_304A, IPC_304B, IPC_305, IPC_306, IPC_307, IPC_308, IPC_309, IPC_310, IPC_311, IPC_312, IPC_313, IPC_314, IPC_315, IPC_316, IPC_317, IPC_318, IPC_319, IPC_320, IPC_321, IPC_322, IPC_323, IPC_324, IPC_325, IPC_326, IPC_326A, IPC_326B, IPC_327, IPC_328, IPC_329, IPC_330, IPC_331, IPC_332, IPC_333, IPC_334, IPC_335, IPC_336, IPC_337, IPC_338, IPC_339, IPC_340, IPC_341, IPC_342, IPC_343, IPC_344, IPC_345, IPC_346, IPC_347, IPC_348, IPC_349, IPC_350, IPC_351, IPC_352, IPC_353, IPC_354, IPC_354A, IPC_354B, IPC_354C, IPC_354D, IPC_355, IPC_356, IPC_357, IPC_358, IPC_359, IPC_360, IPC_361, IPC_362, IPC_363, IPC_363A, IPC_364, IPC_364A, IPC_365, IPC_366, IPC_366A, IPC_366B, IPC_367, IPC_368, IPC_369, IPC_370, IPC_370A, IPC_371, IPC_372, IPC_373, IPC_374, IPC_375, IPC_376, IPC_376A, IPC_376AB, IPC_376B, IPC_376C, IPC_376D, IPC_376DA, IPC_376DB, IPC_376E, IPC_377, IPC_378, IPC_379, IPC_380, IPC_381, IPC_382, IPC_383, IPC_384, IPC_385, IPC_386, IPC_387, IPC_388, IPC_389, IPC_390, IPC_391, IPC_392, IPC_393, IPC_394, IPC_395, IPC_396, IPC_397, IPC_398, IPC_399, IPC_400, IPC_401, IPC_402, IPC_403, IPC_404, IPC_405, IPC_406, IPC_407, IPC_408, IPC_409, IPC_410, IPC_411, IPC_412, IPC_413, IPC_414, IPC_415, IPC_416, IPC_417, IPC_418, IPC_419, IPC_420, IPC_421, IPC_422, IPC_423, IPC_424, IPC_425, IPC_426, IPC_427, IPC_428, IPC_429, IPC_430, IPC_431, IPC_432, IPC_433, IPC_434, IPC_435, IPC_436, IPC_437, IPC_438, IPC_439, IPC_440, IPC_441, IPC_442, IPC_443, IPC_444, IPC_445, IPC_446, IPC_447, IPC_448, IPC_449, IPC_450, IPC_451, IPC_452, IPC_453, IPC_454, IPC_455, IPC_456, IPC_457, IPC_458, IPC_459, IPC_460, IPC_461, IPC_462, IPC_463, IPC_464, IPC_465, IPC_466, IPC_467, IPC_468, IPC_469, IPC_470, IPC_471, IPC_472, IPC_473, IPC_474, IPC_475, IPC_476, IPC_477, IPC_477A, IPC_478, IPC_479, IPC_480, IPC_481, IPC_482, IPC_483, IPC_484, IPC_485, IPC_486, IPC_487, IPC_488, IPC_489, IPC_489A, IPC_489B, IPC_489C, IPC_489D, IPC_489E, IPC_490, IPC_491, IPC_492, IPC_493, IPC_494, IPC_495, IPC_496, IPC_497, IPC_498, IPC_498A, IPC_499, IPC_500, IPC_501, IPC_502, IPC_503, IPC_504, IPC_505, IPC_506, IPC_507, IPC_508, IPC_509, IPC_510, IPC_511\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/pranjalmishra/Desktop/python/ipc_sections.csv', encoding='unicode_escape')\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocessing function using NLTK\n",
    "def preprocessingDesc(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply preprocessing to descriptions in the dataset\n",
    "df['Description'] = df['Description'].map(lambda s: preprocessingDesc(s))\n",
    "\n",
    "# Tokenize and encode each description using BERT, returning a single vector (mean of all tokens)\n",
    "def bert_vectorize(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Take the mean of the last hidden layer's outputs across tokens\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "user_question = input(\"Please enter your text: \")\n",
    "print(\"You entered:\", user_question)\n",
    "\n",
    "preprocessed_question = preprocessingDesc(user_question)\n",
    "question_vector = bert_vectorize(preprocessed_question)\n",
    "\n",
    "description_vectors = np.array([bert_vectorize(desc) for desc in df.Description])\n",
    "\n",
    "similarities = cosine_similarity([question_vector], description_vectors)\n",
    "\n",
    "similarity_threshold = 0.1\n",
    "\n",
    "similar_indices = np.where(similarities[0] >= similarity_threshold)[0]\n",
    "\n",
    "recommended_ipcs = df.loc[similar_indices, 'Section']\n",
    "\n",
    "num_ipcs_recommended = len(recommended_ipcs)\n",
    "\n",
    "print(f\"Number of Recommended IPC(s): {num_ipcs_recommended}\")\n",
    "print(\"Recommended IPC(s):\", ', '.join(map(str, recommended_ipcs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
